{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 1.4039 - accuracy: 0.6541 - val_loss: 0.6947 - val_accuracy: 0.8493\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.5666 - accuracy: 0.8574 - val_loss: 0.4345 - val_accuracy: 0.8857\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 0.4262 - accuracy: 0.8841 - val_loss: 0.3640 - val_accuracy: 0.8999\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 87us/step - loss: 0.3716 - accuracy: 0.8967 - val_loss: 0.3294 - val_accuracy: 0.9063\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.3398 - accuracy: 0.9042 - val_loss: 0.3070 - val_accuracy: 0.9148\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 3s 69us/step - loss: 0.3173 - accuracy: 0.9105 - val_loss: 0.2907 - val_accuracy: 0.9173\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2994 - accuracy: 0.9147 - val_loss: 0.2766 - val_accuracy: 0.9208\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2847 - accuracy: 0.9185 - val_loss: 0.2646 - val_accuracy: 0.9248\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 3s 62us/step - loss: 0.2724 - accuracy: 0.9221 - val_loss: 0.2548 - val_accuracy: 0.9283\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 3s 66us/step - loss: 0.2612 - accuracy: 0.9254 - val_loss: 0.2474 - val_accuracy: 0.9286\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 3s 71us/step - loss: 0.2511 - accuracy: 0.9282 - val_loss: 0.2379 - val_accuracy: 0.9321\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2417 - accuracy: 0.9301 - val_loss: 0.2309 - val_accuracy: 0.9348\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2333 - accuracy: 0.9332 - val_loss: 0.2265 - val_accuracy: 0.9348\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 3s 67us/step - loss: 0.2257 - accuracy: 0.9359 - val_loss: 0.2186 - val_accuracy: 0.9367\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.2184 - accuracy: 0.9378 - val_loss: 0.2118 - val_accuracy: 0.9383\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.2116 - accuracy: 0.9395 - val_loss: 0.2059 - val_accuracy: 0.9415\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 94us/step - loss: 0.2052 - accuracy: 0.9413 - val_loss: 0.2008 - val_accuracy: 0.9435\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 6s 132us/step - loss: 0.1990 - accuracy: 0.9433 - val_loss: 0.1961 - val_accuracy: 0.9448\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.1933 - accuracy: 0.9446 - val_loss: 0.1920 - val_accuracy: 0.9457\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 120us/step - loss: 0.1880 - accuracy: 0.9468 - val_loss: 0.1873 - val_accuracy: 0.9477\n",
      "10000/10000 [==============================] - 1s 55us/step\n",
      "Test score: 0.18530682515799998\n",
      "Test accuracy: 0.947700023651123\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved \n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in \n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#following book to establish accuracy rate data sets with two hidden layers\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 250\n",
    "BATCH_SIZE = 128 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation \n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in \n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 9s 188us/step - loss: 1.2066 - accuracy: 0.6211 - val_loss: 0.4657 - val_accuracy: 0.8845\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 9s 186us/step - loss: 0.5780 - accuracy: 0.8230 - val_loss: 0.3384 - val_accuracy: 0.9066\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 9s 179us/step - loss: 0.4635 - accuracy: 0.8624 - val_loss: 0.2857 - val_accuracy: 0.9183\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 8s 169us/step - loss: 0.4031 - accuracy: 0.8820 - val_loss: 0.2552 - val_accuracy: 0.9260\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 0.3671 - accuracy: 0.8915 - val_loss: 0.2324 - val_accuracy: 0.9323\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 9s 181us/step - loss: 0.3337 - accuracy: 0.9023 - val_loss: 0.2134 - val_accuracy: 0.9371\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.3101 - accuracy: 0.9095 - val_loss: 0.1997 - val_accuracy: 0.9409\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 8s 173us/step - loss: 0.2871 - accuracy: 0.9157 - val_loss: 0.1865 - val_accuracy: 0.9467\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 9s 182us/step - loss: 0.2722 - accuracy: 0.9201 - val_loss: 0.1761 - val_accuracy: 0.9488\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 9s 185us/step - loss: 0.2543 - accuracy: 0.9246 - val_loss: 0.1682 - val_accuracy: 0.9517\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.2448 - accuracy: 0.9280 - val_loss: 0.1609 - val_accuracy: 0.9538\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 9s 196us/step - loss: 0.2381 - accuracy: 0.9304 - val_loss: 0.1543 - val_accuracy: 0.9553\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 8s 175us/step - loss: 0.2249 - accuracy: 0.9337 - val_loss: 0.1490 - val_accuracy: 0.9567\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 7s 142us/step - loss: 0.2160 - accuracy: 0.9365 - val_loss: 0.1433 - val_accuracy: 0.9582\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 9s 180us/step - loss: 0.2111 - accuracy: 0.9385 - val_loss: 0.1402 - val_accuracy: 0.9589\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 7s 153us/step - loss: 0.2050 - accuracy: 0.9386 - val_loss: 0.1359 - val_accuracy: 0.9605\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 12s 252us/step - loss: 0.1966 - accuracy: 0.9422 - val_loss: 0.1312 - val_accuracy: 0.9613\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 12s 244us/step - loss: 0.1911 - accuracy: 0.9438 - val_loss: 0.1274 - val_accuracy: 0.9628\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 10s 200us/step - loss: 0.1845 - accuracy: 0.9451 - val_loss: 0.1254 - val_accuracy: 0.9625\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 8s 159us/step - loss: 0.1800 - accuracy: 0.9473 - val_loss: 0.1220 - val_accuracy: 0.9643\n",
      "10000/10000 [==============================] - 0s 38us/step\n",
      "Test score: 0.12077148525044322\n",
      "Test accuracy: 0.963699996471405\n"
     ]
    }
   ],
   "source": [
    "#testing different batch sizes (50)\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 50\n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation \n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in \n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 1.2177 - accuracy: 0.7084 - val_loss: 0.5690 - val_accuracy: 0.8667\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 5s 102us/step - loss: 0.4942 - accuracy: 0.8700 - val_loss: 0.3913 - val_accuracy: 0.8940\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 5s 108us/step - loss: 0.3896 - accuracy: 0.8916 - val_loss: 0.3363 - val_accuracy: 0.9064\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 5s 114us/step - loss: 0.3451 - accuracy: 0.9023 - val_loss: 0.3085 - val_accuracy: 0.9137\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 5s 104us/step - loss: 0.3173 - accuracy: 0.9100 - val_loss: 0.2886 - val_accuracy: 0.9188\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.2975 - accuracy: 0.9149 - val_loss: 0.2732 - val_accuracy: 0.9237\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.2808 - accuracy: 0.9207 - val_loss: 0.2593 - val_accuracy: 0.9267\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2664 - accuracy: 0.9247 - val_loss: 0.2477 - val_accuracy: 0.9300\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 5s 109us/step - loss: 0.2545 - accuracy: 0.9277 - val_loss: 0.2376 - val_accuracy: 0.9337\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 82us/step - loss: 0.2433 - accuracy: 0.9306 - val_loss: 0.2312 - val_accuracy: 0.9352\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 84us/step - loss: 0.2332 - accuracy: 0.9339 - val_loss: 0.2214 - val_accuracy: 0.9383\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 6s 116us/step - loss: 0.2238 - accuracy: 0.9361 - val_loss: 0.2145 - val_accuracy: 0.9413\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 4s 77us/step - loss: 0.2153 - accuracy: 0.9392 - val_loss: 0.2113 - val_accuracy: 0.9420\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.2076 - accuracy: 0.9410 - val_loss: 0.2024 - val_accuracy: 0.9446\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.2001 - accuracy: 0.9433 - val_loss: 0.1953 - val_accuracy: 0.9466\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.1933 - accuracy: 0.9449 - val_loss: 0.1907 - val_accuracy: 0.9487\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 5s 105us/step - loss: 0.1869 - accuracy: 0.9470 - val_loss: 0.1857 - val_accuracy: 0.9490\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 5s 101us/step - loss: 0.1808 - accuracy: 0.9490 - val_loss: 0.1810 - val_accuracy: 0.9497\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 5s 98us/step - loss: 0.1754 - accuracy: 0.9503 - val_loss: 0.1774 - val_accuracy: 0.9516\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 6s 122us/step - loss: 0.1703 - accuracy: 0.9513 - val_loss: 0.1730 - val_accuracy: 0.9520\n",
      "10000/10000 [==============================] - 0s 47us/step\n",
      "Test score: 0.1739636492960155\n",
      "Test accuracy: 0.9498999714851379\n"
     ]
    }
   ],
   "source": [
    "#Testing different batch sizes (100)\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20 \n",
    "BATCH_SIZE = 100 \n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved \n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in \n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 2.0546 - accuracy: 0.3186 - val_loss: 1.5759 - val_accuracy: 0.7106\n",
      "Epoch 2/20\n",
      "48000/48000 [==============================] - 4s 83us/step - loss: 1.3926 - accuracy: 0.5996 - val_loss: 0.9064 - val_accuracy: 0.8110\n",
      "Epoch 3/20\n",
      "48000/48000 [==============================] - 3s 70us/step - loss: 1.0007 - accuracy: 0.6995 - val_loss: 0.6426 - val_accuracy: 0.8541\n",
      "Epoch 4/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.8179 - accuracy: 0.7486 - val_loss: 0.5294 - val_accuracy: 0.8709\n",
      "Epoch 5/20\n",
      "48000/48000 [==============================] - 4s 78us/step - loss: 0.7207 - accuracy: 0.7790 - val_loss: 0.4685 - val_accuracy: 0.8813\n",
      "Epoch 6/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.6530 - accuracy: 0.8016 - val_loss: 0.4273 - val_accuracy: 0.8893\n",
      "Epoch 7/20\n",
      "48000/48000 [==============================] - 4s 75us/step - loss: 0.6064 - accuracy: 0.8169 - val_loss: 0.3994 - val_accuracy: 0.8935\n",
      "Epoch 8/20\n",
      "48000/48000 [==============================] - 4s 73us/step - loss: 0.5721 - accuracy: 0.8289 - val_loss: 0.3762 - val_accuracy: 0.8982\n",
      "Epoch 9/20\n",
      "48000/48000 [==============================] - 4s 79us/step - loss: 0.5406 - accuracy: 0.8388 - val_loss: 0.3597 - val_accuracy: 0.9008\n",
      "Epoch 10/20\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.5194 - accuracy: 0.8428 - val_loss: 0.3445 - val_accuracy: 0.9045\n",
      "Epoch 11/20\n",
      "48000/48000 [==============================] - 4s 74us/step - loss: 0.5011 - accuracy: 0.8512 - val_loss: 0.3319 - val_accuracy: 0.9083\n",
      "Epoch 12/20\n",
      "48000/48000 [==============================] - 4s 93us/step - loss: 0.4836 - accuracy: 0.8567 - val_loss: 0.3207 - val_accuracy: 0.9097\n",
      "Epoch 13/20\n",
      "48000/48000 [==============================] - 5s 106us/step - loss: 0.4625 - accuracy: 0.8626 - val_loss: 0.3113 - val_accuracy: 0.9122\n",
      "Epoch 14/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.4527 - accuracy: 0.8667 - val_loss: 0.3028 - val_accuracy: 0.9147\n",
      "Epoch 15/20\n",
      "48000/48000 [==============================] - 4s 81us/step - loss: 0.4371 - accuracy: 0.8703 - val_loss: 0.2956 - val_accuracy: 0.9161\n",
      "Epoch 16/20\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.4290 - accuracy: 0.8735 - val_loss: 0.2875 - val_accuracy: 0.9177\n",
      "Epoch 17/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.4178 - accuracy: 0.8764 - val_loss: 0.2810 - val_accuracy: 0.9190\n",
      "Epoch 18/20\n",
      "48000/48000 [==============================] - 4s 85us/step - loss: 0.4083 - accuracy: 0.8803 - val_loss: 0.2751 - val_accuracy: 0.9208\n",
      "Epoch 19/20\n",
      "48000/48000 [==============================] - 4s 76us/step - loss: 0.3983 - accuracy: 0.8825 - val_loss: 0.2689 - val_accuracy: 0.9222\n",
      "Epoch 20/20\n",
      "48000/48000 [==============================] - 4s 80us/step - loss: 0.3890 - accuracy: 0.8849 - val_loss: 0.2632 - val_accuracy: 0.9232\n",
      "10000/10000 [==============================] - 1s 63us/step\n",
      "Test score: 0.2665637231171131\n",
      "Test accuracy: 0.9229999780654907\n"
     ]
    }
   ],
   "source": [
    "#Testing different batch sizes (250)\n",
    "from __future__ import print_function \n",
    "import numpy as np \n",
    "from keras.datasets import mnist \n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Dropout, Activation \n",
    "from keras.optimizers import SGD \n",
    "from keras.utils import np_utils \n",
    "np.random.seed(1671) # for reproducibility \n",
    "# network and training \n",
    "NB_EPOCH = 20\n",
    "BATCH_SIZE = 250\n",
    "VERBOSE = 1 \n",
    "NB_CLASSES = 10 # number of outputs = number digits\n",
    "OPTIMIZER = SGD() # optimizer, explained later in this chapter\n",
    "N_HIDDEN = 128 \n",
    "\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN is reserved for validation \n",
    "DROPOUT = 0.3\n",
    "# data: shuffled and split between train and test \n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "#X_train is 60000 rows of 28x28 values --> reshaped in \n",
    "RESHAPED = 784 \n",
    "X_train = X_train.reshape(60000, RESHAPED) \n",
    "X_test = X_test.reshape(10000, RESHAPED) \n",
    "X_train = X_train.astype('float32') \n",
    "X_test = X_test.astype('float32') \n",
    "# normalize \n",
    "X_train /= 255 \n",
    "X_test /= 255 \n",
    "print(X_train.shape[0], 'train samples') \n",
    "print(X_test.shape[0], 'test samples') \n",
    "# convert class vectors to binary class matrices \n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES) \n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES) \n",
    "# M_HIDDEN hidden layers \n",
    "# 10 outputs \n",
    "# final stage is softmax \n",
    "model = Sequential() \n",
    "model.add(Dense(N_HIDDEN, input_shape=(RESHAPED,))) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(N_HIDDEN)) \n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(DROPOUT))\n",
    "model.add(Dense(NB_CLASSES)) \n",
    "model.add(Activation('softmax')) \n",
    "model.summary() \n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=OPTIMIZER, \n",
    "              metrics=['accuracy']) \n",
    "history = model.fit(X_train, Y_train, \n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH, \n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n",
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE) \n",
    "print(\"Test score:\", score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test different parameters that affect the training and testing accuracy, I adjusted the batch size for three different tests. I first ran a test with a batch size of 50 and received a training accuracy of 94.73 and a testing accuracy 96.36 indicating that a higher epoch would be necessary gain better accuracy at this batch size. I then tested a batch size of 100 and got a training accuracy of 95.13 and testing accuracy 94.98. The training accuracy should be higher than the testing, so this is a step in the right direction but still a lower score than it could be. I then tested a batch size of 250 and got a training accuracy of 88.49 and a testing accuracy of 92.3. This was a significant backslide compared to the the lesser batch sizes. Comparing to the test using a batch size of 128 and a epoch 250, where the training accuracy is 91.54 and the testing accuracy is 94.25, these results indicate that a higher epoch is needed to gain higher training accuracy as a higher batch size is not enough and weakens the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
